{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/tutorials_notebooks_in_class_2024/W02_Intro_to_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gzHo80DX3J0"
   },
   "source": [
    "# Introduction to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVB8QC6bX3J5"
   },
   "source": [
    "We would like to introduce you to scikit-learn with the help of an instructional example about text classification. We will cover the most basic principles and ideas about scikit-learn in this notebook. This tutorial is inspired by the sklearn tutorial on http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html, but contains a few more explanations and is suited to introduce scikit-learn in class.\n",
    "\n",
    "$Author$: Phillip Str√∂bel\n",
    "\n",
    "With minor adjustments from: Janis Goldzycher, Andrianos Michail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuLQDR5MX3J6"
   },
   "source": [
    "## Data\n",
    "\n",
    "Get the data from http://qwone.com/~jason/20Newsgroups/. We will work with the 20news-bydate.tar.gz data set. Unzip it to a suitable destination. Here, all the data lies in the data folder. To our convenience, it has already been split into a training and a test set, so we don't have to care about this. What we need to do though is get the data and put it into a dataframe (you could also only work with dictionaries or other data containers). We do this for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_df(path_to_data, random_state=42):\n",
    "    \"\"\"\n",
    "    Takes the path of a folder containing all the subfolders (which contain the actual documents).\n",
    "    Builds a pandas datafram with document ids, the text and the label.\n",
    "    :param path_to_data: path to top folder as a string\n",
    "    :param random_state: integer, seed for shuffling\n",
    "    :return: pandas dataframe with all the data\n",
    "    \"\"\"\n",
    "    doc_list = list()  # doc_list now: [[doc<str>, label<str>], ...]\n",
    "\n",
    "    for category in os.listdir(path_to_data):\n",
    "        for document in os.listdir(os.path.join(path_to_data, category)):\n",
    "            doc = open(os.path.join(path_to_data, category, document), 'r', encoding='latin-1').read().replace('\\n', ' ')\n",
    "            doc_list.append([doc, category])\n",
    "\n",
    "    df = pd.DataFrame(doc_list, columns=['text', 'label'])\n",
    "\n",
    "    return df.sample(frac=1, random_state=random_state) # return and shuffle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf 20news-bydate.tar.gz\n",
    "!echo \"The files are unzipped and this folder now contains:\"\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mv 20news-bydate-* data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_df('data/20news-bydate-train')\n",
    "test = create_df('data/20news-bydate-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvWeXL2X3J8"
   },
   "source": [
    "Several ways to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training size: ', train.shape)\n",
    "print('test size: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpIz2S4PX3KA"
   },
   "source": [
    "As usual, we split the labels from the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "y_train = train.label\n",
    "X_test = test.text\n",
    "y_test = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmuRdUSSX3KC"
   },
   "source": [
    "Series is just a \"One-dimensional ndarray with axis labels\". Let's see if we got this right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test set shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyUIWl1BX3KD"
   },
   "source": [
    "## Preprocessing\n",
    "So far, so good! But we know that machine learning algorithms cannot work with text data directly. So we need to vectorise the data somehow. also, we might do some preprocessing. Let's see how we can tackle these problems.\n",
    "### Vectorise the data\n",
    "Luckily, sklearn offers some nice classes which help us. We should tokenise the data and then vectorise it. Conveniently, sklearns `CountVectoriser()` does exactly that. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(encoding='latin-1')\n",
    "X_train_counts = count_vect.fit_transform(X_train)  # num_docs x num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jINuDaIX3KD"
   },
   "source": [
    "Basically, the three central methods in sklearn are `transform`, `fit`, `fit_transform`, and `predict`. We will see how each of these work and when to use them. We have alredy made use of `fit_transform`. Instead of using this method, we could have called the method `fit` on the training set first and the use `transform` to vectorise the data (to 'transform' it). With the fitted `CountVectorizer` we can now transform other data, like for example the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9P64uoyX3KE"
   },
   "source": [
    "We will return to this later. First let us see what `CountVectorizer` produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-d5NODYX3KF"
   },
   "source": [
    "The vectorised form contains 11314 rows, which is the number of our documents, while the number of columns tells us something about the vocabulary size of the whole corpus. But what's a sparse matrix? Note that saving the complete, sparse document-vocabulary matrix would need to hold 1,472,030,598 values, most of which would be zero? Why? Instead, we only save 1,787,565 values in a compressed sparse row format. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "row_indices = np.array([0, 0, 1, 2, 2, 2])\n",
    "col_indices = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sparse.csr_matrix((data, (row_indices, col_indices)), shape=(3, 3))\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mtx.todense()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIBL28e0X3KG"
   },
   "source": [
    "How does indexing of sparse matrices work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtx[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f68k9dGPX3KG"
   },
   "source": [
    "Now let's apply our new knowledge to our word-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn2DMP8HX3KH"
   },
   "source": [
    "We can see which positions of the document vector are occupied. A `1` means the word occurs once in the document, while any other number gives the exact count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA0pHM24X3KH"
   },
   "source": [
    "The number of words in a document is also trivial to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBm5u0qCX3KH"
   },
   "source": [
    "In a similar fashion, we can count how many times a certain word occurs in the training set. (In this case, the word occurring first in the vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKwmL2JlX3KH"
   },
   "source": [
    "We can also learn more about the vocabulary, e.g., how many times a word occurs in the corpus. First, we need to find the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get('sin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odmWGpRHX3KI"
   },
   "source": [
    "Now we have the index, we can count how many times the word \"sin\" occurs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_index = count_vect.vocabulary_.get('sin')\n",
    "X_train_counts[:,sin_index].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlSBsNpoX3KI"
   },
   "source": [
    "So far, so good. `CountVectorizer` lets you also define if you want to count bigrams, or other n-grams. Moreover, you can not only count words, but als characters. We suggest you try these out for yourself. In the following, we will continue with unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BuVj8CmX3KI"
   },
   "source": [
    "Since we have numbers now instead of strings, we could start training models now. However, raw counts will not be very informative, since we also have to take the length of a dodument into account. Dividing each row by the total number of words will give us the term frequency for each document. That will be much better! Now we still might have higher values for words which occur often in many documents. typically, these words are less informative, so we need to downscale those weights. This will modify or counts so that we are left with what is called the \"term frequency-inverse document frequency\" measure, or tf-idf. The tf-idf measure is given by\n",
    "\\begin{equation}\n",
    "f_{t,d}\\cdot log \\frac{N}{n_t}\n",
    "\\end{equation}\n",
    "In sklearn, there is the `TfidfTransformer` which does exactly that for us :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_tranformer = TfidfTransformer(smooth_idf=True).fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_tranformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JRtlsIlX3KJ"
   },
   "source": [
    "Again we apply the transformation to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_tranformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyveNTbUX3KJ"
   },
   "source": [
    "This should suffice as features to train a classifer (for the moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ6F8ZM1X3KJ"
   },
   "source": [
    "### Vectorise labels\n",
    "Next, we deal with the labels. Every document has exactly one label attached. We have 20 labels in total. This means we can basically assign a number to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOpydQgDX3KK"
   },
   "source": [
    "## Finally, let's train models\n",
    "Now it's time to train models. Let's stick to the Multinomial Naive Bayes classifier for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY8jOFswX3KL"
   },
   "source": [
    "Let's see how well we do on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSrwY3o8X3KL"
   },
   "source": [
    "Computing the accuracy is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(nb_clf.predict(X_test_tfidf)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(nb_clf.predict(X_test_tfidf), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcTQCrM_X3KM"
   },
   "source": [
    "Almost 80 percent, that is not too bad. What about a Support Vector Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(svc.predict(X_test_tfidf)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxGpuK1pX3KM"
   },
   "source": [
    "An increase of 8%, that's good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qckHMB-X3KM"
   },
   "source": [
    "However, in order to determine the performance of our models we need cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(nb_clf, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svc, X_train_tfidf, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1_rQidWX3KN"
   },
   "source": [
    "We can also calculate precision, recall, and f1 relatively easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=50)\n",
    "sgd_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_train_predictions = cross_val_predict(sgd_clf, X_train_tfidf, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_train, y_train_predictions, average='micro'))\n",
    "print(recall_score(y_train, y_train_predictions, average='micro'))\n",
    "print(f1_score(y_train, y_train_predictions, average='micro'))\n",
    "conf_mx = confusion_matrix(y_train, y_train_predictions)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br-EQ2AoX3KO"
   },
   "source": [
    "## Shortcuts in sklearn - pipelines\n",
    "Sklearn allows us to build convenient `Pipelines`, which facilitate the management of our data and the training of our models enourmously. Consider for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('nb_clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx1_FDKcX3KO"
   },
   "source": [
    "We could even replace the two first lines of the pipeline by using `TfidfVectorizer`, which first fits and transforms the input the same way as the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(text_clf, X_train, y_train, scoring='accuracy', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKm1CyaX3KP"
   },
   "source": [
    "## Model selection - find your best model\n",
    "For every model you would like to train, there is a plethora of parameters you could set. How to find the best model? Again, sklearn has a solution: `GridSearchCV`. With grid search cross validation, you can set your hyperparameter space and train different models with all the parameter combinations. Keep in mind that depending on how many folds you train, the whole training procedure takes significantly longer. But let's set up grid search cross validation. We set up a new pipeline for a SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'svc__loss': ['hinge', 'squared_hinge'],\n",
    "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
    "\n",
    "gs_svc = GridSearchCV(text_svc, param_grid, cv=5, n_jobs=4, verbose=1)\n",
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd3PtMjYX3KP"
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "text_svc = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "             'svc__loss': ['hinge', 'squared_hinge'],\n",
    "             'svc__multi_class': ['ovr', 'crammer_singer']}\n",
    "\n",
    "gs_svc = GridSearchCV(text_svc, param_grid, cv=10, n_jobs=3, verbose=1, return_train_score=True)\n",
    "gs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_df = pd.DataFrame.from_dict(gs_svc.cv_results_)\n",
    "svc_df.sort_values(by=[\"rank_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', LinearSVC(loss='hinge', multi_class='crammer_singer'))\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, prediction in enumerate(best_model.predict(X_test)):\n",
    "    if prediction == y_test[index]:\n",
    "        correct +=1\n",
    "\n",
    "print('Accuracy: ', correct/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBLEzh-MCVfe"
   },
   "source": [
    "##  Modern Solutions Sneak Peek - Transformer\n",
    "\n",
    "Let's look at another task, paraphrase detection. Do two sentences have the same meaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 1. Load Pre-trained Model and Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"AMHR/adversarial-paraphrasing-detector\"  # Replace with the model you want to use\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Prepare the Dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 3. Evaluate the Model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for i, example in enumerate(eval_dataset):\n",
    "        if i > 5:\n",
    "           break\n",
    "        # Tokenize the inputs and get the model's predictions\n",
    "        inputs = tokenizer(example['sentence1'], example['sentence2'], return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        # Move input tensors to the same device as the model\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Print first 5 example pairs along with predictions and ground truth labels\n",
    "        if i < 5:\n",
    "            print(f\"Example {i+1}\")\n",
    "            print(f\"Sentence 1: {example['sentence1']}\")\n",
    "            print(f\"Sentence 2: {example['sentence2']}\")\n",
    "            print(f\"Prediction: {'Paraphrase' if predictions == 1 else 'Not a Paraphrase'}\")\n",
    "            print(f\"Ground Truth: {'Paraphrase' if example['label'] == 1 else 'Not a Paraphrase'}\")\n",
    "            print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_paraphrase(sentence1, sentence2, model, tokenizer, device):\n",
    "    # Prepare the sentences for the model\n",
    "    inputs = tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Move the input tensors to the device the model is on\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get model's prediction\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Paraphrase\" if prediction == 1 else \"Not a Paraphrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Sentences testing\n",
    "sentence1 = \"This tutorial rocks.\"\n",
    "sentence2 = \"I want to throw rocks at this Tutor.\"\n",
    "\n",
    "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
    "print(f\"Sentence 1: {sentence1}\")\n",
    "print(f\"Sentence 2: {sentence2}\")\n",
    "print(f\"Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Sentences Testing\n",
    "sentence1 = \"I am so tired but I want to stay in this tutorial.\"\n",
    "sentence2 = \"I am exhausted and forced to be here.\"\n",
    "\n",
    "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
    "print(f\"Sentence 1: {sentence1}\")\n",
    "print(f\"Sentence 2: {sentence2}\")\n",
    "print(f\"Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Sentences Testing\n",
    "sentence1 = \"This field of research is pretty cool.\"\n",
    "sentence2 = \"I find this line of research very cool.\"\n",
    "\n",
    "result = predict_paraphrase(sentence1, sentence2, model, tokenizer, device)\n",
    "print(f\"Sentence 1: {sentence1}\")\n",
    "print(f\"Sentence 2: {sentence2}\")\n",
    "print(f\"Prediction: {result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
